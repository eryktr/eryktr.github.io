Title: Floating point numbers in memory - the IEE754 standard
Date: 06-27-2022
Category: Software Engineering
Tags: software-engineering, programming, math, computer-science
Slug: iee754
Author: eryktr
Summary: Have you ever wondered how a floating point number like *3.14159* is internally stored in computer memory?
    This article explains the industry standard solution to this problem - the IEE754.

# Problem statement
As every engineer knows, each object that exists in memory, eventually has to be broken down into individual building blocks - bits.
For that matter, let's focus on floating point numbers. As an example, consider a few leading digits of the $\pi$ number:
$$\pi \approx 3.14159$$
How can we store a value like that in memory? 
The most natural thing that might come to your mind would be this:

> Let's store the integer part and the fractional part separately as integers.

This idea would, of course, work but there are a bunch of problems with this attempt that pretty much disqualify
this approach from generic industry-level solutions where performance and space-efficiency is key.

1. We would have to know the length of the fractional part in advance. We might be working with numbers like $3.1$ as well as with $0.000012451005$
    Where do we store the length? Do we fix the value? If so, we would be wasting space for some numbers like $0.5$ and might run out of it when 
    we need more precision.
2. Such implementation would certainly cost more memory and as a result require more CPU cycles to load or store the values. Currently, there are two main
    implementations of floating point numbers - the **float** (32 bits / 4 bytes) and **double** (64 bits / 8 bytes).
    We wouldn't be able to meet that with our approach.

Keep reading to find what the industry standard is.

# Recap - integers in memory
Before we tackle on the more complex problem, let's make sure we are on the same page and understand the basics. 
Let's review the information about integers first. If you find this chapter too trivial, just go ahead and move over to the next one.

## Decimal integers
Let's have a look at some decimal (base 10) and inspect how they are formed.

$$
\begin{align}
42 
&= 4 \cdot 10 + 2 \cdot 1 \\
&= 4 \cdot 10^1 + 2 \cdot 10^0
\end{align}
$$

And a bigger one:

$$
\begin{align}
12345 
&= 1 \cdot 10,000 + 2 \cdot 1,000 + 3 \cdot 100 + 4 \cdot 10 + 5 \cdot 1 &\\
&= 1 \cdot 10^4 + 2 \cdot 10^3 + 3 \cdot 10^2 + 4 \cdot 10^1 + 5 \cdot 10^0 &\\
\end{align}
$$

I think now you see where it all goes. 

> Each decimal integer is represented as a sum of the factors of the form $A \cdot 10 ^ n$, where $0 \leq A \leq 9$

Now, let's transition to the binary system.

## Binary integers
Remember what we reviewed about decimal integers in the previous chapter? The same rule applies to binary integers. With a few tweaks.

1. $10$ is no longer the base of the system - with binary integers, it's $2$
2. We no longer have $10$ digits - we only have $2$ of them

Therefore we can conclude:

> Each binary integer is represented as a sum of the factors of the form $A \cdot 2 ^n$, where $0 \leq A \leq 1$

Examples:

$$
\begin{align}
1101 &= 1 \cdot 2^3 + 1 \cdot 2^2 + 0 \cdot 2^1 + 1 \cdot 2^0 \\
&= 8 + 4 + 0 + 1 = 13
\end{align}
$$

And another one

$$
\begin{align}
10000 &= 1 \cdot 2^4 + 0 \cdot 2^3 + \ldots + 0 \cdot 2^0 = 16  
\end{align}
$$

## Signed vs unsigned integers
When writing code, you might come across two technical terms - **signed integers** and **unsigned integers**. 
Signed integers can store both positive and negative numbers, whereas unsigned integers can only store non-negative numbers.

## Integer size
Apart from being signed or unsigned, numeric types are also divided based on their size (the number of bits they take). The most common ones are **8 bits**, **32 bits** and **64 bits**.

## Common numeric types
Putting that together, these are the most common integer data types that you can encounter:

* **uint8** (aka **byte**) - unsigned integer, using 8 bits
* **int32** - signed integer, using 32 bits
* **uint32** - unsigned integer, using 32 bits
* **int64**
* **uint64**

## Closer look at the byte
This is the simplest numeric type. In other words, this is an unsigned binary integer composed of eight bits.
In other words, the binary number is eight digits long. Each of these digits can either be $0$ or $1$. 
The most significant bit (the leftmost one) corresponds to the value of $2^7 = 128$.

Sample byte:
$$
01001101 = 1 + 4 + 8 + 64 =  77
$$

### Question: What's the lowest number a byte can store?
Of course, we set all of the bits to $0$ s. The answer is

$$
00000000 = 0
$$

### Question: What's the largest number a byte can store?
Conversely, this time we set all of the bits to $1$ s. Which yields:

$$
11111111 = 1 + 2 + 4 + 8 + 16 + 32 + 64 + 128 = 255
$$

### Question: How do you encode your age into a byte?
Let's say you are $25$ years old. Let's break that number into a sum of distinct powers of two.

$$
25 = 16 + 8 + 1 = 2^4 + 2^3 + 2^0
$$

So, the bits corresponding to the powers of $0$, $3$ and $4$ need to be set.

$$
00011001
$$

## How about **uint32**?
The datatypes of **uint32** and **uint64** work in exactly the same way as the byte. The only difference is the number of bits that go into the number.
They use **32** and **64** bits respectively.

For example, that's a number encoded using the **uint32** data type.

$$
100000001 00000001 00000000 00000001
$$

What's the value of that number?

### Question: What's the largest number you can encode using a **uint32**?
Set all of the bits to $1$. As a result, we are getting the following sum:

$$
2^0 + 2^1 + 2^2 + \ldots + 2^30 + 2^31 = 2^32 - 1 = 4,294,967,295
$$


## What about the sign?

# Floating point numbers in memory - rationale
# IEE754
# Summary